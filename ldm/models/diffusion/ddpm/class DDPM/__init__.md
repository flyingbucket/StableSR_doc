# init å‡½æ•°ä»‹ç»

```python
class DDPM(pl.LightningModule): # [[pytorch lightning#pl.LightningMoudule]]
    # classic DDPM with Gaussian diffusion, in image space
    def __init__(self,
                 unet_config,
                 timesteps=1000,
                 beta_schedule="linear",
                 loss_type="l2",
                 ckpt_path=None,
                 ignore_keys=[],
                 load_only_unet=False,
                 monitor="val/loss",
                 use_ema=True,
                 first_stage_key="image",
                 image_size=256,
                 channels=3,
                 log_every_t=100,
                 clip_denoised=True,
                 linear_start=1e-4,
                 linear_end=2e-2,
                 cosine_s=8e-3,
                 given_betas=None,
                 original_elbo_weight=0.,
                 v_posterior=0.,  # weight for choosing posterior variance as sigma = (1-v) * beta_tilde + v * beta
                 l_simple_weight=1.,
                 conditioning_key=None,
                 parameterization="eps",  # all assuming fixed variance schedules
                 scheduler_config=None,
                 use_positional_encodings=False,
                 learn_logvar=False,
                 logvar_init=0.,
                 ):
        super().__init__() 
        # [[#parameterization]]
        assert parameterization in ["eps", "x0", "v"], 'currently only supporting "eps" and "x0" and "v"'
        self.parameterization = parameterization
        print(f"{self.__class__.__name__}: Running in {self.parameterization}-prediction mode")
        self.cond_stage_model = None
        self.clip_denoised = clip_denoised
        self.log_every_t = log_every_t
        self.first_stage_key = first_stage_key
        self.image_size = image_size  # try conv?
        self.channels = channels
        self.use_positional_encodings = use_positional_encodings
        self.model = DiffusionWrapper(unet_config, conditioning_key) # [[DiffusionWrapper(pl.LightningModule)]]
        count_params(self.model, verbose=True) # [[#line 45 to 57 ğŸ”§ `DDPM.__init__` ä¸­æ¨¡å‹ç®¡ç†ã€è°ƒåº¦å™¨ä¸æŸå¤±æƒé‡éƒ¨åˆ†è§£æ]]
        self.use_ema = use_ema
        if self.use_ema:
            self.model_ema = LitEma(self.model)
            print(f"Keeping EMAs of {len(list(self.model_ema.buffers()))}.")

        self.use_scheduler = scheduler_config is not None
        if self.use_scheduler:
            self.scheduler_config = scheduler_config

        self.v_posterior = v_posterior
        self.original_elbo_weight = original_elbo_weight
        self.l_simple_weight = l_simple_weight

        if monitor is not None: # [[#ğŸ“ˆ PyTorch Lightning ä¸­çš„ `monitor` å‚æ•°ç®€æ]]
            self.monitor = monitor 
        if ckpt_path is not None: # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys, only_model=load_only_unet) # [[init_from_ckpt]]
		# [[#line 64 to end]]
		# [[#1. æ³¨å†Œè°ƒåº¦è¡¨ï¼ˆbeta scheduleï¼‰]]
        self.register_schedule(given_betas=given_betas, beta_schedule=beta_schedule, timesteps=timesteps,
                               linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)
		# [[#2. è®¾ç½®æŸå¤±å‡½æ•°ç±»å‹]]
        self.loss_type = loss_type
		# [[#logvar åœ¨ DDPM æ‰©æ•£æ¨¡å‹ä¸­çš„ä½œç”¨ä¸å®ç°]]
        self.learn_logvar = learn_logvar
        self.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))
        if self.learn_logvar:
            self.logvar = nn.Parameter(self.logvar, requires_grad=True)

```



## parameterization

- `parameterization`: æ‰©æ•£æ¨¡å‹çš„é¢„æµ‹ç›®æ ‡ï¼Œæœ‰ä¸‰ç§ï¼š
    - `"eps"`: å™ªå£°é¢„æµ‹ï¼ˆæœ€å¸¸ç”¨ï¼‰
    - `"x0"`: é¢„æµ‹åŸå§‹å›¾åƒ
    - `"v"`: v-pred æ–¹æ¡ˆï¼Œå¹³è¡¡ä¸¤ä¸ªæç«¯
- æ–­è¨€é™åˆ¶åªæ”¯æŒä¸Šè¿°ä¸‰ç§æ¨¡å¼

## line 45 to 57 ğŸ”§ `DDPM.__init__` ä¸­æ¨¡å‹ç®¡ç†ã€è°ƒåº¦å™¨ä¸æŸå¤±æƒé‡éƒ¨åˆ†è§£æ

è¿™éƒ¨åˆ†ä»£ç è´Ÿè´£æ‰©æ•£æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„å‡ ä¸ªé‡è¦åŠŸèƒ½ç»„ä»¶çš„é…ç½®ï¼ŒåŒ…æ‹¬å‚æ•°ç»Ÿè®¡ã€EMA å¹³æ»‘ã€è°ƒåº¦å™¨è®¾ç½®ï¼Œä»¥åŠæŸå¤±é¡¹çš„åŠ æƒç­–ç•¥ã€‚

---

### ğŸ”¢ æ¨¡å‹å‚æ•°ç»Ÿè®¡ä¸æ‰“å°

```python
count_params(self.model, verbose=True)
```

- è°ƒç”¨ `count_params` æ‰“å°æ¨¡å‹å‚æ•°æ•°é‡ï¼›
- `self.model` æ˜¯ä¹‹å‰åˆ›å»ºçš„ `DiffusionWrapper`ï¼ˆåŒ…å« UNet å’Œæ¡ä»¶æ§åˆ¶ï¼‰ï¼›
- `verbose=True` è¡¨ç¤ºè¾“å‡ºè¯¦ç»†å±‚çº§å‚æ•°ç»Ÿè®¡ï¼Œæœ‰åŠ©äºæ¨¡å‹è°ƒè¯•ä¸è§„æ¨¡è¯„ä¼°ã€‚

---

### ğŸ§® EMA æ¨¡å‹é…ç½®ï¼ˆExponential Moving Averageï¼‰

```python
self.use_ema = use_ema
if self.use_ema:
    self.model_ema = LitEma(self.model)
    print(f"Keeping EMAs of {len(list(self.model_ema.buffers()))}.")
```

- `use_ema`: æ§åˆ¶æ˜¯å¦å¯ç”¨ EMAï¼›
- å¦‚æœå¯ç”¨ï¼Œå°†åˆ›å»º `model_ema`ï¼Œç”¨äºåœ¨è®­ç»ƒä¸­å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œæ»‘åŠ¨å¹³å‡ï¼›
- EMA å¯åœ¨æ¨ç†æ—¶æä¾›æ›´ç¨³å®šçš„ç»“æœï¼ˆå°¤å…¶è®­ç»ƒåæœŸï¼‰ï¼›
- `LitEma` æ˜¯ä¸€ä¸ªå†…éƒ¨å®ç°çš„ EMA å·¥å…·ç±»ï¼ˆæ¨¡ä»¿ PyTorch EMA å®ç°ï¼‰ï¼›
- `buffers()` æä¾›äº†æ‰€æœ‰è¢« EMA è¿½è¸ªçš„å¼ é‡ï¼ˆé€šå¸¸æ˜¯æ¨¡å‹æƒé‡ï¼‰ã€‚

PS. [EMA](https://zhuanlan.zhihu.com/p/68748778)ç®€ä»‹
æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆExponential Moving Averageï¼‰ä¹Ÿå«æƒé‡ç§»åŠ¨å¹³å‡ï¼ˆWeighted Moving Averageï¼‰ï¼Œæ˜¯ä¸€ç§ç»™äºˆè¿‘æœŸæ•°æ®æ›´é«˜æƒé‡çš„å¹³å‡æ–¹æ³•ã€‚


### ğŸ“ˆ è®­ç»ƒè°ƒåº¦å™¨é…ç½®ï¼ˆå¦‚å­¦ä¹ ç‡è°ƒåº¦ï¼‰

```python
self.use_scheduler = scheduler_config is not None
if self.use_scheduler:
    self.scheduler_config = scheduler_config
```

- å¦‚æœæä¾›äº† `scheduler_config`ï¼Œåˆ™å°†å…¶ä¿å­˜ï¼›
- åç»­åœ¨ `configure_optimizers()` æ–¹æ³•ä¸­ä¼šç”¨åˆ°è¯¥é…ç½®ï¼›
- å¯ç”¨äºå®šä¹‰å¦‚ä½™å¼¦é€€ç«ï¼ˆcosine annealingï¼‰ã€çº¿æ€§ warmup ç­‰å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ã€‚

---

### âš–ï¸ æŸå¤±é¡¹æƒé‡è®¾ç½®

```python
self.v_posterior = v_posterior
self.original_elbo_weight = original_elbo_weight
self.l_simple_weight = l_simple_weight
```

è¿™ä¸‰é¡¹å‚æ•°æ§åˆ¶æ‰©æ•£æŸå¤±çš„ç»„æˆï¼š

| å‚æ•°å                    | å«ä¹‰                                                                                           |
| ---------------------- | -------------------------------------------------------------------------------------------- |
| `v_posterior`          | åéªŒæ–¹å·®çš„åŠ æƒæ§åˆ¶é¡¹ã€‚ç”¨äºè®¾ç½®é¢„æµ‹æ–¹å·®çš„ç­–ç•¥ã€‚å…·ä½“è®¡ç®—å½¢å¼ä¸ºï¼š<br>`ÏƒÂ² = (1 - v) * beta_tilde + v * beta`ï¼Œ<br>å…¶ä¸­ `v` å°±æ˜¯è¿™ä¸ªå‚æ•°ã€‚ |
| `original_elbo_weight` | æ˜¯å¦åŠ å…¥åŸå§‹è®ºæ–‡ä¸­çš„ ELBO loss é¡¹ï¼ˆå¸¸ä¸º 0ï¼Œä»£è¡¨ä¸å¯ç”¨ï¼‰                                                           |
| `l_simple_weight`      | L2 æˆ– L1 æŸå¤±çš„ä¸»æƒé‡ï¼Œç”¨äºç¨³å®šè®­ç»ƒ                                                                        |

- è¿™éƒ¨åˆ†æœ€ç»ˆå°†ä½œç”¨äº `get_loss()` æˆ– `p_losses()` ä¸­çš„æŸå¤±å‡½æ•°ç»„åˆï¼›
- å¯¹äºä¸åŒä»»åŠ¡ï¼ˆå¦‚å›¾åƒé‡å»º vs ç”Ÿæˆï¼‰ï¼Œå¯ä»¥é€šè¿‡è°ƒæ•´è¿™äº›å‚æ•°æ¥å¹³è¡¡ç”Ÿæˆè´¨é‡å’Œä¿çœŸåº¦ã€‚

---


### âœ… å°ç»“

è¿™ä¸€éƒ¨åˆ†ä¸ºè®­ç»ƒè¿‡ç¨‹æä¾›äº†å¿…è¦çš„é…ç½®ç®¡ç†ï¼š
- **æ¨¡å‹å‚æ•°ç»Ÿè®¡**æœ‰åŠ©äºå¯è§†åŒ–è§„æ¨¡ï¼›
- **EMA ç®¡ç†**å¯æå‡è®­ç»ƒç¨³å®šæ€§ï¼›
- **è°ƒåº¦å™¨è®¾ç½®**ä¸ºä¼˜åŒ–å™¨è¡Œä¸ºæä¾›äº†çµæ´»æ€§ï¼›
- **æŸå¤±é¡¹åŠ æƒ**æ§åˆ¶ç”Ÿæˆæ¨¡å‹ä¼˜åŒ–ç›®æ ‡çš„ä¾§é‡ç‚¹ã€‚


## ğŸ“ˆ PyTorch Lightning ä¸­çš„ `monitor` å‚æ•°ç®€æ

`monitor` æ˜¯ PyTorch Lightning ä¸­å›è°ƒï¼ˆCallbackï¼‰æœºåˆ¶çš„ä¸€éƒ¨åˆ†ï¼Œç”¨äºæŒ‡å®š**è®­ç»ƒè¿‡ç¨‹ä¸­è¦ç›‘æ§çš„æŒ‡æ ‡åç§°**ï¼Œä¾›å¦‚ `ModelCheckpoint`ã€`EarlyStopping` ç­‰å›è°ƒä¾æ®è¯¥æŒ‡æ ‡æ‰§è¡Œç›¸åº”é€»è¾‘ï¼ˆå¦‚ä¿å­˜æ¨¡å‹ã€æå‰åœæ­¢ç­‰ï¼‰ã€‚
è¯¦è§[[pytorch lightning#pytorch_Lightning Callback æœºåˆ¶]]

---

### âœ… å…³é”®ç”¨é€”

| ç»„ä»¶                | ç”¨é€”             |
| ----------------- | -------------- |
| `ModelCheckpoint` | ä¿å­˜æ€§èƒ½æœ€å¥½çš„æ¨¡å‹      |
| `EarlyStopping`   | åœ¨éªŒè¯æŒ‡æ ‡åœæ­¢æå‡æ—¶ä¸­æ­¢è®­ç»ƒ |

---

### ğŸ§© monitor çš„å·¥ä½œæµç¨‹

1. **æ¨¡å‹å†…éƒ¨è®°å½•æŒ‡æ ‡**ï¼š
   ```python
   self.log("val/loss", val_loss, prog_bar=True)
   ```

2. **æŒ‡å®š monitor çš„å›è°ƒç›‘å¬è¿™ä¸ªæŒ‡æ ‡**ï¼š
   ```python
   ModelCheckpoint(monitor="val/loss", mode="min")
   ```

3. **Lightning è‡ªåŠ¨æ¯”è¾ƒå¹¶è§¦å‘ä¿å­˜ / åœæ­¢é€»è¾‘**ã€‚

---

### âš™ï¸ monitor ç¤ºä¾‹é…ç½®

```python
from pytorch_lightning.callbacks import ModelCheckpoint

checkpoint = ModelCheckpoint(
    monitor="val/psnr",   # ç›‘å¬ PSNR æŒ‡æ ‡
    mode="max",           # æŒ‡æ ‡è¶Šå¤§è¶Šå¥½
    save_top_k=1,
    filename="best-psnr"
)
```

```python
from pytorch_lightning.callbacks import EarlyStopping

early_stop = EarlyStopping(
    monitor="val/loss",
    mode="min",
    patience=5
)
```

---

### ğŸ§  è¯´æ˜

- `monitor` æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¿…é¡»å’Œ `.log(...)` ä¸­è®°å½•çš„åå­—ä¸€è‡´ï¼›
- ä¸ä¼šè‡ªåŠ¨åˆ›å»ºæŒ‡æ ‡å€¼ï¼Œåªæ˜¯å¼•ç”¨å·²æœ‰æŒ‡æ ‡ï¼›
- å’Œ `mode` ä¸€èµ·å†³å®šä½•æ—¶è§¦å‘æ“ä½œï¼ˆ`min` â†’ è¶Šå°è¶Šå¥½ï¼Œ`max` â†’ è¶Šå¤§è¶Šå¥½ï¼‰ï¼›
- åœ¨æ¨¡å‹ç±»ä¸­å¯ç”¨ `self.monitor` ä¼ é€’ç»™ callback å®ç°åŠ¨æ€é…ç½®ã€‚

---

### âœ… æ€»ç»“

- `monitor` æ˜¯ **å›è°ƒç³»ç»Ÿç›‘å¬çš„æŒ‡æ ‡åç§°**ï¼›
- é…åˆ `.log(...)` ä½¿ç”¨ï¼›
- å†³å®šæ˜¯å¦ä¿å­˜æ¨¡å‹ / æå‰åœæ­¢ï¼›
- æœ¬èº«ä¸è®¡ç®—æŒ‡æ ‡ï¼Œä»…ä½œä¸ºå¼•ç”¨å­—æ®µä½¿ç”¨ã€‚


## line 64 to end

è¿™ä¸€éƒ¨åˆ†ä¸»è¦å®Œæˆäº†å™ªå£°è°ƒåº¦ä¸æŸå¤±é…ç½®,å…·ä½“æ¥è¯´,
æœ¬éƒ¨åˆ†ä»£ç å®Œæˆäº†æ‰©æ•£è¿‡ç¨‹ä¸­çš„**betaè°ƒåº¦è¡¨æ„å»º**(å³å™ªå£°è°ƒåº¦)ã€**æŸå¤±å‡½æ•°ç±»å‹è®¾å®š**å’Œ**å¯¹æ•°æ–¹å·®çš„åˆå§‹åŒ–**ï¼Œæ˜¯ DDPM å»ºæ¨¡æ ¸å¿ƒå‚æ•°çš„å…³é”®é…ç½®æ­¥éª¤ã€‚

---

### 1. æ³¨å†Œè°ƒåº¦è¡¨ï¼ˆbeta scheduleï¼‰

```python
self.register_schedule(
    given_betas=given_betas,
    beta_schedule=beta_schedule,
    timesteps=timesteps,
    linear_start=linear_start,
    linear_end=linear_end,
    cosine_s=cosine_s
)
```

- è¯¥å‡½æ•°æ ¹æ® `beta_schedule` çš„ç±»å‹ï¼ˆå¦‚ "linear" æˆ– "cosine"ï¼‰æ„å»ºæ‰©æ•£è¿‡ç¨‹ä¸­çš„æ—¶é—´æ­¥å™ªå£°ç³»æ•°è¡¨ï¼›
- é€šå¸¸ä¼šç”Ÿæˆå¦‚ä¸‹å‚æ•°ï¼š
  - `betas`ï¼šæ¯ä¸€æ­¥çš„å™ªå£°å¹…åº¦ï¼›
  - `alphas`, `alphas_cumprod`, `sqrt_alphas_cumprod`, `sqrt_one_minus_alphas_cumprod` ç­‰ï¼›
- è¿™äº›ç³»æ•°ä¼šåœ¨åç»­ `q_sample`, `q_posterior`, `predict_start_from_noise` ç­‰å‡½æ•°ä¸­ä½¿ç”¨ï¼›
- å‚æ•°è¯´æ˜ï¼š
  - `linear_start`, `linear_end`: ç”¨äºçº¿æ€§ beta è°ƒåº¦èµ·æ­¢å€¼ï¼›
  - `cosine_s`: ç”¨äºè°ƒæ•´ä½™å¼¦è°ƒåº¦çš„å½¢çŠ¶ï¼›
  - `given_betas`: è‹¥æä¾›ï¼Œä¼˜å…ˆä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰ beta è¡¨ã€‚

---

### 2. è®¾ç½®æŸå¤±å‡½æ•°ç±»å‹

```python
self.loss_type = loss_type
```

- æ§åˆ¶è®­ç»ƒæ—¶ä½¿ç”¨å“ªç§æŸå¤±ï¼š
  - `"l2"`ï¼ˆé»˜è®¤ï¼‰ï¼šé¢„æµ‹çš„å™ªå£°ä¸çœŸå®å™ªå£°ä¹‹é—´çš„å‡æ–¹è¯¯å·®ï¼›
  - `"l1"`ï¼šé¢„æµ‹æ®‹å·®çš„ç»å¯¹å€¼æŸå¤±ï¼›
  - ä¹Ÿå¯èƒ½æ”¯æŒå…¶ä»–è‡ªå®šä¹‰æŸå¤±ç±»å‹ï¼Œå¦‚ perceptual lossã€hybrid loss ç­‰ï¼›
- å®é™…ä½¿ç”¨åœ¨ `get_loss()` æˆ– `p_losses()` ä¸­å¤„ç†ã€‚

---

### 3. åˆå§‹åŒ–å¯¹æ•°æ–¹å·®ï¼ˆlog-varianceï¼‰

```python
self.learn_logvar = learn_logvar
self.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))
if self.learn_logvar:
    self.logvar = nn.Parameter(self.logvar, requires_grad=True)
```

- `logvar` æ˜¯ä¸€ä¸ªé•¿åº¦ä¸º `num_timesteps` çš„å¼ é‡ï¼Œè¡¨ç¤ºæ¯ä¸€æ‰©æ•£æ—¶é—´æ­¥çš„å¯¹æ•°æ–¹å·®ï¼›
- å¦‚æœå¯ç”¨ `learn_logvar=True`ï¼Œåˆ™å°†å…¶è®¾ä¸ºå¯å­¦ä¹ å‚æ•°ï¼ˆ`nn.Parameter`ï¼‰ï¼Œå…è®¸æ¨¡å‹è‡ªåŠ¨ä¼˜åŒ–æ¯ä¸€æ—¶é—´æ­¥çš„ä¸ç¡®å®šæ€§ï¼›
- å¦‚æœä¸å¯ç”¨ï¼Œ`logvar` å°±æ˜¯ä¸€ä¸ªå›ºå®šçš„å¸¸é‡å€¼å¼ é‡ï¼›
- åœ¨ `get_loss()` ä¸­ä¼šå‚ä¸ KL é¡¹æˆ– likelihood çš„æƒé‡è°ƒæ•´ã€‚
#### logvar åœ¨ DDPM æ‰©æ•£æ¨¡å‹ä¸­çš„ä½œç”¨ä¸å®ç°

`logvar`ï¼ˆå¯¹æ•°æ–¹å·®ï¼‰æ˜¯æ‰©æ•£æ¨¡å‹ï¼ˆå¦‚ DDPMï¼‰ä¸­ç”¨äºæ§åˆ¶è®­ç»ƒæŸå¤±æƒé‡å’Œä¸ç¡®å®šæ€§å»ºæ¨¡çš„ä¸€ä¸ªé‡è¦å˜é‡ã€‚åœ¨ StableSR å’Œ DDPM å®ç°ä¸­ï¼Œå®ƒæ˜¯ diffusion æ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œè€Œé encoder æˆ– decoder çš„ç»„æˆéƒ¨åˆ†ã€‚

---

##### 1. èƒŒæ™¯ï¼šä¸ºä½•éœ€è¦ logvar

æ‰©æ•£æ¨¡å‹è®­ç»ƒæ—¶é€šå¸¸ä»¥é¢„æµ‹å™ªå£°ä¸ºç›®æ ‡ï¼ŒåŸºæœ¬æŸå¤±å½¢å¼ä¸º(ä»¥L2æŸå¤±ä¸ºä¾‹)ï¼š
$$
L_t = \left\| \varepsilon_{\text{pred}} - \varepsilon_{\text{true}} \right\|^2
$$
ä¸ºäº†å¢å¼ºçµæ´»æ€§ã€ç¨³å®šæ€§æˆ–é€¼è¿‘å¯¹æ•°ä¼¼ç„¶ï¼Œä¸€äº›å˜ä½“å¼•å…¥äº†å¯¹æ•°æ–¹å·® logvarï¼Œä½¿å¾—æŸå¤±å‡½æ•°å˜ä¸ºï¼š
$$
L_t = \frac{1}{2} \cdot \exp(-\text{logvar}_t) \cdot \left\| \varepsilon_{\text{pred}} - \varepsilon_{\text{true}} \right\|^2 + \frac{1}{2} \cdot \text{logvar}_t
$$
è¿™ç›¸å½“äºä½¿ç”¨ä¸€ä¸ªå¯å˜çš„æ—¶é—´æ­¥æƒé‡é¡¹ï¼Œç”¨äºï¼š
- æ§åˆ¶æ¯ä¸€æ—¶é—´æ­¥æŸå¤±çš„ç›¸å¯¹é‡è¦æ€§ï¼›
- æ¨¡æ‹Ÿé«˜æ–¯ä¼¼ç„¶ä¸­çš„åˆ†å¸ƒä¸ç¡®å®šæ€§ï¼›
- ä½¿å¾—æ¨¡å‹å¯¹æŸäº›æ—¶é—´æ­¥é¢„æµ‹æ›´åŠ ç¨³å¥ã€‚

---

##### 2. åˆå§‹åŒ–æ–¹å¼

logvar é€šå¸¸è¢«åˆå§‹åŒ–ä¸ºå¸¸æ•°å¼ é‡ï¼š

```python
self.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))
```

- `logvar_init`: å¯¹æ•°æ–¹å·®çš„åˆå§‹å€¼ï¼ˆå¸¸ä¸º 0ï¼‰ï¼›
- `num_timesteps`: æ‰©æ•£æ€»æ­¥æ•°ï¼ˆå¦‚ 1000ï¼‰ï¼›
- å¾—åˆ°å½¢çŠ¶ä¸º `(T,)` çš„ logvar å¼ é‡ï¼Œå…¶ä¸­ T æ˜¯æ—¶é—´æ­¥æ•°ã€‚

è‹¥å¯ç”¨å¯å­¦ä¹ æ–¹å·®ï¼š

```python
if self.learn_logvar:
    self.logvar = nn.Parameter(self.logvar, requires_grad=True)
```

åˆ™è¯¥å¼ é‡åœ¨è®­ç»ƒä¸­ä¼šè‡ªåŠ¨æ›´æ–°ï¼Œæ¯ä¸€æ—¶é—´æ­¥éƒ½æœ‰ä¸åŒçš„å¯å­¦ä¹ ä¸ç¡®å®šæ€§ã€‚

---

##### 3. ä½¿ç”¨æ–¹å¼ï¼ˆè®­ç»ƒæ—¶ï¼‰

logvar é€šå¸¸å‚ä¸æŸå¤±å‡½æ•°å®šä¹‰ï¼Œåœ¨ `get_loss()` æˆ– `p_losses()` ä¸­ç”¨ä½œåŠ¨æ€æƒé‡ï¼š

```python
loss = weighted_mse / torch.exp(self.logvar[t]) + self.logvar[t]
```

æˆ–æ›´å¤æ‚çš„ï¼š
```python
loss = loss_weight * loss_raw + offset * logvar[t]
```

---

##### 4. logvaræ€»ç»“

| é¡¹ç›®    | å†…å®¹                             |
| ----- | ------------------------------ |
| åç§°    | logvarï¼ˆå¯¹æ•°æ–¹å·®ï¼‰                   |
| ç±»å‹    | Tensor / nn.Parameter          |
| ç»´åº¦    | `(num_timesteps,)`             |
| ç”¨é€”    | æ§åˆ¶ä¸åŒæ—¶é—´æ­¥çš„æŸå¤±æƒé‡ä¸ä¸ç¡®å®šæ€§å»ºæ¨¡            |
| åˆå§‹åŒ–æ–¹æ³• | `torch.full(size, fill_value)` |
| æ˜¯å¦å¯è®­ç»ƒ | ç”± `learn_logvar` å‚æ•°æ§åˆ¶          |

logvar æ˜¯ä¸€ä¸ªæ‰©æ•£è¿‡ç¨‹ä¸­çš„æƒé‡è°ƒèŠ‚å™¨ï¼Œå°¤å…¶åœ¨åŠ å…¥ ELBOã€VLB ç­‰ç›®æ ‡æ—¶å°¤ä¸ºå…³é”®ã€‚


---

## æ€»ç»“

| é¡¹ç›®                        | åŠŸèƒ½è¯´æ˜                            |
| ------------------------- | ------------------------------- |
| `register_schedule()`     | æ„é€ æ‰©æ•£è¿‡ç¨‹ä¸­æ¯ä¸€æ­¥çš„ beta å‚æ•°ï¼Œç”¨äºæ§åˆ¶åŠ å™ªè¿‡ç¨‹    |
| `loss_type`               | å†³å®šè®­ç»ƒæ—¶çš„æŸå¤±å‡½æ•°ç±»å‹ï¼Œå¦‚ L2 æˆ– L1          |
| `learn_logvar` å’Œ `logvar` | æ§åˆ¶æ˜¯å¦å­¦ä¹ æ¯ä¸ªæ—¶é—´æ­¥çš„å¯¹æ•°æ–¹å·®ï¼Œä»¥é€‚é…ä¸åŒçš„ä¸ç¡®å®šæ€§å»ºæ¨¡ç­–ç•¥ |

è¿™äº›è®¾ç½®æ„æˆäº†æ‰©æ•£æ¨¡å‹è®­ç»ƒé˜¶æ®µçš„æ ¸å¿ƒæ•°å­¦åŸºç¡€ã€‚







