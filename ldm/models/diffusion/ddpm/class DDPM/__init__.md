# init å‡½æ•°ä»‹ç»

```python
class DDPM(pl.LightningModule): # [[#pl.LightningMoudule]]
    # classic DDPM with Gaussian diffusion, in image space
    def __init__(self,
                 unet_config,
                 timesteps=1000,
                 beta_schedule="linear",
                 loss_type="l2",
                 ckpt_path=None,
                 ignore_keys=[],
                 load_only_unet=False,
                 monitor="val/loss",
                 use_ema=True,
                 first_stage_key="image",
                 image_size=256,
                 channels=3,
                 log_every_t=100,
                 clip_denoised=True,
                 linear_start=1e-4,
                 linear_end=2e-2,
                 cosine_s=8e-3,
                 given_betas=None,
                 original_elbo_weight=0.,
                 v_posterior=0.,  # weight for choosing posterior variance as sigma = (1-v) * beta_tilde + v * beta
                 l_simple_weight=1.,
                 conditioning_key=None,
                 parameterization="eps",  # all assuming fixed variance schedules
                 scheduler_config=None,
                 use_positional_encodings=False,
                 learn_logvar=False,
                 logvar_init=0.,
                 ):
        super().__init__() 
        # [[#parameterization]]
        assert parameterization in ["eps", "x0", "v"], 'currently only supporting "eps" and "x0" and "v"'
        self.parameterization = parameterization
        print(f"{self.__class__.__name__}: Running in {self.parameterization}-prediction mode")
        self.cond_stage_model = None
        self.clip_denoised = clip_denoised
        self.log_every_t = log_every_t
        self.first_stage_key = first_stage_key
        self.image_size = image_size  # try conv?
        self.channels = channels
        self.use_positional_encodings = use_positional_encodings
        self.model = DiffusionWrapper(unet_config, conditioning_key) # [[DiffusionWrapper(pl.LightningModule)]]
        count_params(self.model, verbose=True) # [[#line 45 to 57 ğŸ”§ `DDPM.__init__` ä¸­æ¨¡å‹ç®¡ç†ã€è°ƒåº¦å™¨ä¸æŸå¤±æƒé‡éƒ¨åˆ†è§£æ]]
        self.use_ema = use_ema
        if self.use_ema:
            self.model_ema = LitEma(self.model)
            print(f"Keeping EMAs of {len(list(self.model_ema.buffers()))}.")

        self.use_scheduler = scheduler_config is not None
        if self.use_scheduler:
            self.scheduler_config = scheduler_config

        self.v_posterior = v_posterior
        self.original_elbo_weight = original_elbo_weight
        self.l_simple_weight = l_simple_weight

        if monitor is not None:
            self.monitor = monitor
        if ckpt_path is not None:
            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys, only_model=load_only_unet)

        self.register_schedule(given_betas=given_betas, beta_schedule=beta_schedule, timesteps=timesteps,
                               linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)

        self.loss_type = loss_type

        self.learn_logvar = learn_logvar
        self.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))
        if self.learn_logvar:
            self.logvar = nn.Parameter(self.logvar, requires_grad=True)

```

## pl.LightningMoudule 

`pl.LightningModule` æ˜¯å¯¹ `torch.nn.Module` çš„é«˜çº§å°è£…
åªéœ€è¦å®ç°ä¸‹é¢å‡ ä¸ªå…³é”®æ–¹æ³•ï¼Œå®ƒå°±èƒ½å¸®ä½ è‡ªåŠ¨å®Œæˆè®­ç»ƒæµç¨‹ã€GPU åˆ†å‘ã€æ—¥å¿—è®°å½•ã€checkpointä¿å­˜ç­‰å¤æ‚æ“ä½œï¼š

- 1. `__init__(self)`: åˆå§‹åŒ–æ¨¡å‹ç»“æ„ã€æŸå¤±å‡½æ•°ã€è¶…å‚æ•°ç­‰ã€‚
    
- 2. `forward(self, x)`: å®šä¹‰**å‰å‘ä¼ æ’­**é€»è¾‘ï¼ˆæ³¨æ„ï¼šä»…åœ¨è°ƒç”¨ `model(x)` æ—¶ä½¿ç”¨ï¼Œ**è®­ç»ƒé€»è¾‘ç”¨ `training_step`**ï¼‰ã€‚
    
- 3. `training_step(self, batch, batch_idx)`:å®šä¹‰**ä¸€ä¸ªè®­ç»ƒæ­¥éª¤**çš„è¡Œä¸ºï¼Œè¿”å› `loss`ã€‚
	- é€šå¸¸çš„æ­¥éª¤æ˜¯
	    - ä» `batch` å–å‡ºæ•°æ®ï¼›
	    - å‰å‘ä¼ æ’­ï¼›
	    - è®¡ç®—æŸå¤±ï¼›
	    - ä½¿ç”¨ `self.log(...)` è‡ªåŠ¨è®°å½•æ—¥å¿—ï¼ˆå¦‚ lossï¼‰ã€‚
        
- 4. `validation_step(...)` / `test_step(...)`:éªŒè¯å’Œæµ‹è¯•é˜¶æ®µçš„è¡Œä¸ºï¼Œç»“æ„ä¸ `training_step` ç±»ä¼¼ã€‚
    
- 5. `configure_optimizers(self)`:è¿”å›ä¼˜åŒ–å™¨å’Œï¼ˆå¯é€‰çš„ï¼‰å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚


## parameterization

- `parameterization`: æ‰©æ•£æ¨¡å‹çš„é¢„æµ‹ç›®æ ‡ï¼Œæœ‰ä¸‰ç§ï¼š
    - `"eps"`: å™ªå£°é¢„æµ‹ï¼ˆæœ€å¸¸ç”¨ï¼‰
    - `"x0"`: é¢„æµ‹åŸå§‹å›¾åƒ
    - `"v"`: v-pred æ–¹æ¡ˆï¼Œå¹³è¡¡ä¸¤ä¸ªæç«¯
- æ–­è¨€é™åˆ¶åªæ”¯æŒä¸Šè¿°ä¸‰ç§æ¨¡å¼

## line 45 to 57 ğŸ”§ `DDPM.__init__` ä¸­æ¨¡å‹ç®¡ç†ã€è°ƒåº¦å™¨ä¸æŸå¤±æƒé‡éƒ¨åˆ†è§£æ

è¿™éƒ¨åˆ†ä»£ç è´Ÿè´£æ‰©æ•£æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„å‡ ä¸ªé‡è¦åŠŸèƒ½ç»„ä»¶çš„é…ç½®ï¼ŒåŒ…æ‹¬å‚æ•°ç»Ÿè®¡ã€EMA å¹³æ»‘ã€è°ƒåº¦å™¨è®¾ç½®ï¼Œä»¥åŠæŸå¤±é¡¹çš„åŠ æƒç­–ç•¥ã€‚

---

### ğŸ”¢ æ¨¡å‹å‚æ•°ç»Ÿè®¡ä¸æ‰“å°

```python
count_params(self.model, verbose=True)
```

- è°ƒç”¨ `count_params` æ‰“å°æ¨¡å‹å‚æ•°æ•°é‡ï¼›
- `self.model` æ˜¯ä¹‹å‰åˆ›å»ºçš„ `DiffusionWrapper`ï¼ˆåŒ…å« UNet å’Œæ¡ä»¶æ§åˆ¶ï¼‰ï¼›
- `verbose=True` è¡¨ç¤ºè¾“å‡ºè¯¦ç»†å±‚çº§å‚æ•°ç»Ÿè®¡ï¼Œæœ‰åŠ©äºæ¨¡å‹è°ƒè¯•ä¸è§„æ¨¡è¯„ä¼°ã€‚

---

### ğŸ§® EMA æ¨¡å‹é…ç½®ï¼ˆExponential Moving Averageï¼‰

```python
self.use_ema = use_ema
if self.use_ema:
    self.model_ema = LitEma(self.model)
    print(f"Keeping EMAs of {len(list(self.model_ema.buffers()))}.")
```

- `use_ema`: æ§åˆ¶æ˜¯å¦å¯ç”¨ EMAï¼›
- å¦‚æœå¯ç”¨ï¼Œå°†åˆ›å»º `model_ema`ï¼Œç”¨äºåœ¨è®­ç»ƒä¸­å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œæ»‘åŠ¨å¹³å‡ï¼›
- EMA å¯åœ¨æ¨ç†æ—¶æä¾›æ›´ç¨³å®šçš„ç»“æœï¼ˆå°¤å…¶è®­ç»ƒåæœŸï¼‰ï¼›
- `LitEma` æ˜¯ä¸€ä¸ªå†…éƒ¨å®ç°çš„ EMA å·¥å…·ç±»ï¼ˆæ¨¡ä»¿ PyTorch EMA å®ç°ï¼‰ï¼›
- `buffers()` æä¾›äº†æ‰€æœ‰è¢« EMA è¿½è¸ªçš„å¼ é‡ï¼ˆé€šå¸¸æ˜¯æ¨¡å‹æƒé‡ï¼‰ã€‚

---

### ğŸ“ˆ è®­ç»ƒè°ƒåº¦å™¨é…ç½®ï¼ˆå¦‚å­¦ä¹ ç‡è°ƒåº¦ï¼‰

```python
self.use_scheduler = scheduler_config is not None
if self.use_scheduler:
    self.scheduler_config = scheduler_config
```

- å¦‚æœæä¾›äº† `scheduler_config`ï¼Œåˆ™å°†å…¶ä¿å­˜ï¼›
- åç»­åœ¨ `configure_optimizers()` æ–¹æ³•ä¸­ä¼šç”¨åˆ°è¯¥é…ç½®ï¼›
- å¯ç”¨äºå®šä¹‰å¦‚ä½™å¼¦é€€ç«ï¼ˆcosine annealingï¼‰ã€çº¿æ€§ warmup ç­‰å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ã€‚

---

### âš–ï¸ æŸå¤±é¡¹æƒé‡è®¾ç½®

```python
self.v_posterior = v_posterior
self.original_elbo_weight = original_elbo_weight
self.l_simple_weight = l_simple_weight
```

è¿™ä¸‰é¡¹å‚æ•°æ§åˆ¶æ‰©æ•£æŸå¤±çš„ç»„æˆï¼š

| å‚æ•°å                    | å«ä¹‰                                                                                           |
| ---------------------- | -------------------------------------------------------------------------------------------- |
| `v_posterior`          | åéªŒæ–¹å·®çš„åŠ æƒæ§åˆ¶é¡¹ã€‚ç”¨äºè®¾ç½®é¢„æµ‹æ–¹å·®çš„ç­–ç•¥ã€‚å…·ä½“è®¡ç®—å½¢å¼ä¸ºï¼š<br>`ÏƒÂ² = (1 - v) * beta_tilde + v * beta`ï¼Œ<br>å…¶ä¸­ `v` å°±æ˜¯è¿™ä¸ªå‚æ•°ã€‚ |
| `original_elbo_weight` | æ˜¯å¦åŠ å…¥åŸå§‹è®ºæ–‡ä¸­çš„ ELBO loss é¡¹ï¼ˆå¸¸ä¸º 0ï¼Œä»£è¡¨ä¸å¯ç”¨ï¼‰                                                           |
| `l_simple_weight`      | L2 æˆ– L1 æŸå¤±çš„ä¸»æƒé‡ï¼Œç”¨äºç¨³å®šè®­ç»ƒ                                                                        |

- è¿™éƒ¨åˆ†æœ€ç»ˆå°†ä½œç”¨äº `get_loss()` æˆ– `p_losses()` ä¸­çš„æŸå¤±å‡½æ•°ç»„åˆï¼›
- å¯¹äºä¸åŒä»»åŠ¡ï¼ˆå¦‚å›¾åƒé‡å»º vs ç”Ÿæˆï¼‰ï¼Œå¯ä»¥é€šè¿‡è°ƒæ•´è¿™äº›å‚æ•°æ¥å¹³è¡¡ç”Ÿæˆè´¨é‡å’Œä¿çœŸåº¦ã€‚

---

## âœ… å°ç»“

è¿™ä¸€éƒ¨åˆ†ä¸ºè®­ç»ƒè¿‡ç¨‹æä¾›äº†å¿…è¦çš„é…ç½®ç®¡ç†ï¼š
- **æ¨¡å‹å‚æ•°ç»Ÿè®¡**æœ‰åŠ©äºå¯è§†åŒ–è§„æ¨¡ï¼›
- **EMA ç®¡ç†**å¯æå‡è®­ç»ƒç¨³å®šæ€§ï¼›
- **è°ƒåº¦å™¨è®¾ç½®**ä¸ºä¼˜åŒ–å™¨è¡Œä¸ºæä¾›äº†çµæ´»æ€§ï¼›
- **æŸå¤±é¡¹åŠ æƒ**æ§åˆ¶ç”Ÿæˆæ¨¡å‹ä¼˜åŒ–ç›®æ ‡çš„ä¾§é‡ç‚¹ã€‚












