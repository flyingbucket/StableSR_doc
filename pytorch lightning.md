# pytorch lightingæ¨¡å—ç®€ä»‹

## pl.LightningMoudule 

`pl.LightningModule` æ˜¯å¯¹ `torch.nn.Module` çš„é«˜çº§å°è£…
åªéœ€è¦å®ç°ä¸‹é¢å‡ ä¸ªå…³é”®æ–¹æ³•ï¼Œå®ƒå°±èƒ½å¸®ä½ è‡ªåŠ¨å®Œæˆè®­ç»ƒæµç¨‹ã€GPU åˆ†å‘ã€æ—¥å¿—è®°å½•ã€checkpointä¿å­˜ç­‰å¤æ‚æ“ä½œï¼š

- 1. `__init__(self)`: åˆå§‹åŒ–æ¨¡å‹ç»“æ„ã€æŸå¤±å‡½æ•°ã€è¶…å‚æ•°ç­‰ã€‚
    
- 2. `forward(self, x)`: å®šä¹‰**å‰å‘ä¼ æ’­**é€»è¾‘ï¼ˆæ³¨æ„ï¼šä»…åœ¨è°ƒç”¨ `model(x)` æ—¶ä½¿ç”¨ï¼Œ**è®­ç»ƒé€»è¾‘ç”¨ `training_step`**ï¼‰ã€‚
    
- 3. `training_step(self, batch, batch_idx)`:å®šä¹‰**ä¸€ä¸ªè®­ç»ƒæ­¥éª¤**çš„è¡Œä¸ºï¼Œè¿”å› `loss`ã€‚
	- é€šå¸¸çš„æ­¥éª¤æ˜¯
	    - ä» `batch` å–å‡ºæ•°æ®ï¼›
	    - å‰å‘ä¼ æ’­ï¼›
	    - è®¡ç®—æŸå¤±ï¼›
	    - ä½¿ç”¨ `self.log(...)` è‡ªåŠ¨è®°å½•æ—¥å¿—ï¼ˆå¦‚ lossï¼‰ã€‚
        
- 4. `validation_step(...)` / `test_step(...)`:éªŒè¯å’Œæµ‹è¯•é˜¶æ®µçš„è¡Œä¸ºï¼Œç»“æ„ä¸ `training_step` ç±»ä¼¼ã€‚
    
- 5. `configure_optimizers(self)`:è¿”å›ä¼˜åŒ–å™¨å’Œï¼ˆå¯é€‰çš„ï¼‰å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚

## pytorch_Lightning Callback æœºåˆ¶

PyTorch Lightning çš„ Callbackï¼ˆå›è°ƒæœºåˆ¶ï¼‰æ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­çš„**äº‹ä»¶é’©å­ç³»ç»Ÿ**ï¼Œå…è®¸ç”¨æˆ·åœ¨è®­ç»ƒ / éªŒè¯ / æµ‹è¯• / ä¿å­˜ç­‰é˜¶æ®µæ’å…¥è‡ªå®šä¹‰é€»è¾‘ï¼Œç±»ä¼¼äºé’©å­ï¼ˆhookï¼‰æˆ–ç›‘å¬å™¨ã€‚

---

### âœ… å¸¸è§ç”¨é€”

- è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆå¦‚æ ¹æ® val/loss æœ€å°ï¼‰
- Early stoppingï¼ˆæå‰åœæ­¢è®­ç»ƒï¼‰
- æ—¥å¿—è®°å½• / å­¦ä¹ ç‡å¯è§†åŒ–
- è‡ªå®šä¹‰æ—¥å¿—ã€è¯„ä¼°ã€æ ·æœ¬å¯è§†åŒ–ç­‰è¡Œä¸º

---

### ğŸ§© æ ¸å¿ƒæ¦‚å¿µï¼šCallback æ˜¯ä¸€ä¸ªç±»

```python
from pytorch_lightning.callbacks import Callback

class MyCallback(Callback):
    def on_train_start(self, trainer, pl_module):
        print("è®­ç»ƒå¼€å§‹ï¼")
    
    def on_validation_end(self, trainer, pl_module):
        print("éªŒè¯é˜¶æ®µç»“æŸã€‚")

    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
        print(f"è®­ç»ƒç¬¬ {batch_idx} ä¸ª batch å®Œæˆ")
```

---

### ğŸ§ª ä½¿ç”¨ Callback çš„æ–¹å¼

```python
from pytorch_lightning import Trainer

trainer = Trainer(callbacks=[MyCallback()])
```

å¯ä»¥ä¸€æ¬¡æ€§æ³¨å†Œå¤šä¸ª callbackï¼š

```python
trainer = Trainer(callbacks=[
    MyCallback(),
    ModelCheckpoint(...),
    EarlyStopping(...)
])
```

---

### ğŸ“¦ å®˜æ–¹å†…ç½®å¸¸ç”¨å›è°ƒ

#### 1. `ModelCheckpoint`: ä¿å­˜æœ€ä¼˜æ¨¡å‹

```python
from pytorch_lightning.callbacks import ModelCheckpoint

checkpoint_callback = ModelCheckpoint(
    monitor="val/loss",     # ç›‘æ§å“ªä¸ªæŒ‡æ ‡
    save_top_k=1,           # åªä¿ç•™ top1
    mode="min",             # ç›®æ ‡æ˜¯æœ€å°åŒ– loss
    filename="best-checkpoint",
    save_last=True
)
```

#### 2. `EarlyStopping`: æå‰åœæ­¢

```python
from pytorch_lightning.callbacks import EarlyStopping

early_stop_callback = EarlyStopping(
    monitor="val/loss",
    patience=5,     # è‹¥ 5 ä¸ª epoch æ²¡æœ‰æå‡åˆ™åœæ­¢
    mode="min"
)
```

---

### ğŸ“š å¸¸ç”¨ Callback é’©å­æ–¹æ³•ä¸€è§ˆ

| æ–¹æ³•å                    | è°ƒç”¨æ—¶æœº            |
| ---------------------- | --------------- |
| `on_fit_start`         | fit() å¼€å§‹æ—¶       |
| `on_train_start`       | è®­ç»ƒé˜¶æ®µå¼€å§‹          |
| `on_train_end`         | è®­ç»ƒé˜¶æ®µç»“æŸ          |
| `on_train_batch_start` | æ¯ä¸ª batch å¼€å§‹å‰    |
| `on_train_batch_end`   | æ¯ä¸ª batch ç»“æŸå    |
| `on_validation_end`    | æ¯è½®éªŒè¯ç»“æŸå         |
| `on_save_checkpoint`   | ä¿å­˜ checkpoint æ—¶ |
| `on_load_checkpoint`   | åŠ è½½ checkpoint æ—¶ |

---

### ğŸ¯ å®ä¾‹ï¼šåœ¨éªŒè¯åè®°å½•å½“å‰æ¨¡å‹çŠ¶æ€

```python
class LogModelNorm(Callback):
    def on_validation_end(self, trainer, pl_module):
        total_norm = 0
        for p in pl_module.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
        total_norm = total_norm ** 0.5
        print(f"å½“å‰æ¢¯åº¦èŒƒæ•°ï¼š{total_norm:.4f}")
```

---

### âœ… å°ç»“

- Callback æ˜¯ä¸€ç§è½»é‡çº§çš„æ’ä»¶ç³»ç»Ÿï¼›
- æ‰€æœ‰æ¨¡å‹ç›¸å…³å›è°ƒéƒ½å¯ä»¥é›†ä¸­ç®¡ç†ï¼Œä¸æ±¡æŸ“æ ¸å¿ƒè®­ç»ƒé€»è¾‘ï¼›
- éå¸¸é€‚åˆè®°å½•æ—¥å¿—ã€ä¿å­˜ä¸­é—´ç»“æœã€åŠ¨æ€ä¿®æ”¹è®­ç»ƒè¡Œä¸ºç­‰ã€‚

